---
title: 'Online Learning'
date: 2025-02-01
permalink: /posts/2025/02/blog-post-1/
tags:

- online learning

---

# online learningⅠ

## FTL and FTRL

### FTL and OSD

Follow The Regularized Leader (FTRL) is a family of algorithms that add a regularization term to Follow The Leader (FTL). When reviewing Follow The Leader (FTL), we mainly discuss an alternative strategy that guarantees sublinear regret under the assumption of a convex setting—namely, Online Subgradient Descent (OSD) and its generalization, Online Mirror Descent (OMD).

OSD is an online algorithm that updates its parameters using the subgradient at each iteration. Its upper bound is generally constrained by the following inequality:
$$
\eta_t (l_t(x_t)-l_t(u)) \leq \eta_t <g_t, x_t-u> \leq \frac{1}{2}||x_t-u||^2_2-\frac{1}{2}||x_{t+1}-u||^2_2+\frac{\eta^2_t}{2}||g_t||^2_2\tag{1}
$$

> the proof can be found in  [Online Gradient Descent – Parameter-free Learning and Optimization Algorithms.pdf](D:/continual_meta_learning/online_learning/basic_knowledge/Online Gradient Descent – Parameter-free Learning and Optimization Algorithms.pdf) P6

and we can get the famous inequality:
$$
\sum^T_{t=1}(l_t(x_t)-l_t(u))\leq \frac{D^2}{2\eta_T}+\sum^T_{t=1}\frac{\eta_t}{2}||g_t||^2_2
$$
and typically, when $\eta_t$ is a constant $\eta$, we obtain:
$$
\sum^T_{t=1}(l_t(x_t)-l_t(u))\leq \frac{||u-x_1||^2_2}{2\eta}+\frac{\eta}{2}\sum^T_{t=1}||g_t||^2_2
$$
And we can minimize the right-hand term using a convex-concave function, and gets $\eta^*$

> the proof can be found in [Online Gradient Descent – Parameter-free Learning and Optimization Algorithms.pdf](D:/continual_meta_learning/online_learning/basic_knowledge/Online Gradient Descent – Parameter-free Learning and Optimization Algorithms.pdf) P6

By removing the dependence on$T$, we can derive a practical online learning algorithm:

```pascal
for t=1 to T:
 output x_{t}
 get l_t
 x_{t+1} = x_{t} - \eta_{t}g_t
 g_t \in \partial l_t(x_t)
end for
```

$\eta_t = \frac{D}{\sqrt{\sum^t_{i=1}||g_i||^2_2}}$

And the algorithm maintains the upper bound, differing only by a constant factor, with the best learning rate.

>the proof can be found in  [Adaptive Algorithms_ L_ bounds and AdaGrad – Parameter-free Learning and Optimization Algorithms.pdf](D:/continual_meta_learning/online_learning/basic_knowledge/Adaptive Algorithms_ L_ bounds and AdaGrad – Parameter-free Learning and Optimization Algorithms.pdf) 

And from Cauchy-Schwarz, we can obtain a lower upper bound in an element-wise sense, which is known as AdaGrad.

```pascal
for t=1 to T:
 output x_t
 get l_t
 g_t \in \partial l_t(x_t)
 for i=1 to d do
  if g_{t,i} ≠ 0
   \eta_{t+1,i} = **
   x_{t+1,i}=max{min{x_{t,i}-\eta_{t,i}g_{t,i},b_{i}},a_{i}}
  else
   x_{t+1,i}=x_{t,i}
  end if
 end for
end for
```

\** = $\frac{\sqrt{2}D_i}{2\sqrt{\sum^i_{j=1}g_{j,i}^2}}$

> the detial can be found in [Adaptive Algorithms_ L_ bounds and AdaGrad – Parameter-free Learning and Optimization Algorithms.pdf](D:/continual_meta_learning/online_learning/basic_knowledge/Adaptive Algorithms_ L_ bounds and AdaGrad – Parameter-free Learning and Optimization Algorithms.pdf) 

We can observe that the learning rates of these algorithms are related to previous gradients, which gives insight into the meaning of regret.

and in $(1)$ we can observe that this is limited by Online Linear Optimization (OLO) using the update equation$x_{t+1}=x_{t}+\eta_tg_t$

here we can discuss more about OLO in  [Lower Bounds for Online Linear Optimization – Parameter-free Learning and Optimization Algorithms.pdf](D:/continual_meta_learning/online_learning/basic_knowledge/Lower Bounds for Online Linear Optimization – Parameter-free Learning and Optimization Algorithms.pdf) 

For practical online gradient descent, when $l_t(x)$ has the property of strong convexity, we can use the following:$\eta_t$：
$$
\eta_t=\frac{1}{\sum^t_{i=1}\mu_i}\\
regret \leq \frac{1}{2}\sum^T_{t=1}{||g_t||^2_2}{\sum^t_{i=1}\mu_i}
$$

> this is discussed in detail in  [More Online-to-Batch Examples and Strong Convexity – Parameter-free Learning and Optimization Algorithms.pdf](D:/continual_meta_learning/online_learning/basic_knowledge/More Online-to-Batch Examples and Strong Convexity – Parameter-free Learning and Optimization Algorithms.pdf) P5

Finally, let's discuss what the OSD algorithm means:
$$
x_{t+1}=\underset{x}{argmin}<g_t,x>+\frac{1}{2\eta_t}||x_t-x||^2_2\tag{2}
$$

### online-to-batch

It's intriguing that we can easily observe how the online subgradient descent algorithm is so similar to the batch algorithm used in deep learning.

The discussion about online-to-batch conversion guarantees the feasibility and the difference between using batch and online learning strategies. Typically, this inequality holds:
$$
\mathbb{E}[F(\frac{1}{\sum^T_{t=1}\alpha_t}\sum^T_{t=1}\alpha_tx_t)]\leq F(u)+\frac{\mathbb{E}[Regret_T(u)]}{\sum^T_{t=1}\alpha_t}
$$

> the detail can be found in  [Anytime Online-to-Batch, Optimism and Acceleration.pdf](D:/continual_meta_learning/online_learning/basic_knowledge/Anytime Online-to-Batch, Optimism and Acceleration.pdf) 

What the upper inequality indicates is that the batch loss is less than the online loss plus an additional term. And the PAC discussion can also be found in [Anytime Online-to-Batch, Optimism and Acceleration.pdf](D:/continual_meta_learning/online_learning/basic_knowledge/Anytime Online-to-Batch, Optimism and Acceleration.pdf) 

### OSD to OMD

somthing about Legendre-Fenchel Transform：
$$
f^*(\theta)=\underset{x\in\R^d}{sup}<\theta,x>-f(x)
$$
Let’s discuss some properties of the Legendre-Fenchel Transform

<a id="space swap">the following are equivalent</a>:

- $\theta\in\partial f(x)$
- $<\theta,y>-f(y)$ achieves its supremum in y at y=x
- $f(x)+f^*(x) = <\theta,x>$
- $x\in\partial f^*(\theta)$



1. $\triangledown f^*(\theta)=x^* $

And let's take a look at the OMD uptate iteration:
$$
x_{t+1}=\triangledown\phi^*(\triangledown\phi(x_t)-\eta_tg_t)\\
=\triangledown\underset{x}{sup}(<\triangledown\phi(x_t)-\eta_tg_t,x>-\phi(x))
$$
which means:
$$
x_{t+1}\in\{x|g_t+\frac{1}{\eta_t}(\triangledown\phi(x)-\triangledown\phi(x_t))\}\tag{3}
$$
Now let's talk about OMD from Bregman Divergence
$$
\mathcal{B}_{\phi}(x,y)=\phi(x)-\phi(y)-<\triangledown\phi(y),x-y>=\frac{1}{2}(x-y)^T\triangledown^2\phi(z)(x-y)\geq\frac{\lambda}{2}||x-y||^2
$$

> OMD is discussed in  [Online Mirror Descent I_ Bregman version – Parameter-free Learning and Optimization Algorithms.pdf](D:/continual_meta_learning/online_learning/basic_knowledge/Online Mirror Descent I_ Bregman version – Parameter-free Learning and Optimization Algorithms.pdf) （Ⅰ to Ⅲ）

similar to $(2)$

we discuss $(3)$ in Bregman Divergence context
$$
x_{t+1} = \underset{x}{argmin}<g_t,x>+\frac{1}{\eta_t}\mathcal{B}_{\phi}(x;x_t)
$$
when $\phi(x)=\frac{1}{2}||x||^2_2$, we obtain OSD in $(1)$

### some examples for OMD

1. exponential gradient

   in this example $\phi$ is defined as $\sum^d_{i=1}x_ilnx_i$. Obviously, it takes the form of entropy.

   after a complex analysis,

   > [Online Mirror Descent III_ Examples and Learning with Expert Advice – Parameter-free Learning and Optimization Algorithms.pdf](D:/continual_meta_learning/online_learning/basic_knowledge/Online Mirror Descent III_ Examples and Learning with Expert Advice – Parameter-free Learning and Optimization Algorithms.pdf) 

   we can obtain:
   $$
   x_{t+1,j}=\frac{x_{t,j}exp(-\eta g_{t,j})}{\sum^d_{i=1}x_{t,i}exp(-\eta g_{t,i})},j=1...d
   $$

Observe that the update follows a constant $\eta$

## FTRL

Now let's discuss FTRL (Follow The Regularized Leader).

First, we can observe the formula for FTRL:
$$
x_t\in argmin_{x\in V}\phi_t(x)+\sum^{t-1}_{i=1}l_i(x)
$$
where $\phi$ represents the regularization term.

Next, we turn our attention to the concept of Regret, which is defined as:
$$
Regret = \sum^{T}_{i=1}(l_i(x)-l_i(x^*))
$$
where $x^*$ is the optimal minimizer, and the regret measures the cumulative difference between the chosen actions and the best possible actions.

The upper bound analysis related to the FTRL Regret can be found in the paper  [Follow-The-Regularized-Leader I_ Regret Equality – Parameter-free Learning and Optimization Algorithms.pdf](D:/continual_meta_learning/online_learning/basic_knowledge/Follow-The-Regularized-Leader I_ Regret Equality – Parameter-free Learning and Optimization Algorithms.pdf) 

In a nutshell, the analysis proceeds as follows:

1. First, we assume that $F_t+l_t$ is $\lambda_t$ strongly convex, where $F_t=\phi_t+\sum^{t-1}_{i=1}l_t$
2. Then, we prove that $f(x)-f(x^*)\leq\frac{1}{2\mu}||g||^2_*$ where $x^*$ is the minimization piont. This result is derived by constructing a helper function $g(z)=f(z)-<g_y,z>$ which makes $y$ the minimization point of $g(z)$

### FTRL with Linearized Losses

In this section, we discuss the general upper bound of the convex loss function $l$.

We start with the following inequality:
$$
l(x_t)-l(u)\leq <g_{x_t}, x_t-u>
$$
Now, considering this linear loss function, the FTRL (Follow The Regularized Leader) algorithm can be written as:
$$
x_{t+1} = argmin_{x\in V}<\sum^t_{i=1}g_i,x>+\phi_{t+1}(x)
$$
from <a href="#space swap">this</a> we can derive the update rule: $x_{t+1}=\triangledown\phi^*_{V,t+1}(-\sum^t_{i=1}g_i)$

Thus, the update process becomes:
$$
\theta_{t+1}=\theta_t-g_t\\
x_{t+1}=\triangledown\phi^*_{V,t+1}(\theta_{t+1})
$$
Finally, we can compare this update process to the Online Mirror Descent (OMD) algorithm.
$$
\theta_{t+1}=\triangledown\phi(x_t)-\eta_tg_t\\
x_{t+1}=\triangledown\phi^*_V(\theta_{t+1})
$$

### from linearized FTRL to OMD

In this section, we analyze the equanlity between linearized FTRL and OMD.

We begin by introducing some assumptions that allow us to derive the same formulation for both linearized FTRL and OMD.

First, let $x_{t+1}\in int\ dom\phi$ for all $t$. This implies that $\eta g_t + \triangledown \phi(x_{t+1})-\triangledown \phi(x_t)=0$. From this, we obtain:
$$
\triangledown \phi(x_{t+1})=-\eta\sum^t_{i=1}g_i
$$
Next, consider FTRL with linearized losses with regularizers $\phi_t=\frac{1}{\eta}\phi$

With this setup, we can derive the same algorithm as in the OMD framework.

## summary

In this section, we summarize the high-level intuition behind the algorithms we use in machine learning, along with the underlying theorems that support them.

Let’s begin with the most familiar algorithm: SGD.

At its core, SGD is an online learning algorithm, and it is a special case of OSD (Online Subgradient Descent), distinguished by its differentiability and batch setting. However, we can use subgradient methods and online-to-batch analysis to encompass both.

When it comes to OSD, it’s clear that it’s an algorithm derived from OMD (Online Mirror Descent), where the L2 norm function plays the role of $\phi$ in Bregman Divergence.

Moreover, it’s important to recognize that $\phi$ represents the regularization term used to smooth the update process while optimizing the output $x$.

On a deeper level, we can explore the differences between various choices of $\phi$ and the impact of using different norms, such as the L1 norm and L2 norm, on the optimization process.
