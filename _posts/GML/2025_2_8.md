# Geometric Priors Ⅰ

## domains and signals

In general, learning in high dimensions is intractable:

- number of samples required grows exponentially with dimension
- In Geometric DL, data are signals on spaces/domains
- Learning becomes tractable via the principles of:
  - Symmetry
  - Scale separation

some sources of error

- approximation
- statistical
- optimization
- using geometric priors
  - improve statistical error
  - not worsen approximation error

Domain $\Omega$ is a set with additional structure.

The space of signal on a geometric domain

- A signal on $\Omega$ is just a function $x:\Omega\rarr\mathcal{C}$
  - $\Omega$ is the domain
  - $\mathcal{C}$ is a vector space, whose dimensions are called cannels.
  - space $\mathcal{C}$-valued signals on $\Omega$ is defined as $\mathcal{X}(\Omega,\mathcal{C})=\{x:\Omega\rarr\mathcal{C}\}$

## Hilbert Space Structure

- We can add signals and multiply by scalars:
  - $(\alpha x+\beta y)(u)=\alpha x(u)+\beta y(u)$
  - $<x,y>=\int_{\Omega}<x(u),y(u)>_{\mathcal{C}}d\mu(u)$ where $\mu$ is a measure on $\Omega$

notes:

- maps $u\in \Omega$ to $x(u)\in \mathcal{C}$
- Field maps $u\in \Omega$ to $x(u)\in \mathcal{C}_u$

somtimes we do not have signals on the domain, but the domain is the data.

- meshes or graphs without node or edge features
- point clouds
  - graph adjacency matrix can be viewed as a signal on $\Omega\times\Omega$
  - Metric tensor can be viewed as a signal on the node $\Omega$

## Symmetric

A Symmetry of an object is a transformation of that object that leaves it unchanged.

Symmetric of the Parameterization

- let $\mathcal{X}$ denote the input space, $\mathcal{Y}$ the label space, and $\mathcal{W}$ the weight space

- let f: $\mathcal{X}\times\mathcal{W}\rarr\mathcal{Y}$ denote a model

- A transformation is a symmetry of the parameterization if 
  $$
  f(x,g,w)=f(x,w)
  $$
  <img src="C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20250208101842245.png" alt="image-20250208101842245" style="zoom:25%;" />

symmetries of the Label Funcion

learning class and learning symmetries



### transformation on a domain

- permutation
- rotation, translation, reflection
- diffeomorphism preserves the smooth structure of a manifold $\Omega$

### the propretry of symmetries

- the collection of all symmetries of a given object is called a symmetry group
  - the identity transformation is always a symmetry
  - given two symmetry transformtions, their composition is also a symmetry
  - given any symmetry, its inverse is also a symmetry

## Abstract Group

- associativity
- identity
- inverse
- closure

### some terms

- symmetry group, a group whose elements are transformations.
- abstract groups: a set of element together with a composition rule
- group actions: a map $·\times\Omega\rarr\Omega$

elements and operations

### kinds of groups

- discrete groups
- continuous & Lie Groups

### symmetries og $\Omega$ acting on Signals $\mathcal{X}(\Omega,\mathcal{C})$

$$
(gx)(u)=x(g^{-1}u)
$$

### Linearity of the Group Action on Signals

$$
g(\alpha x+\beta y)=\alpha gx+\beta gy
$$

### group representations

permutation

## Group and Graph

- the group of permutations
- the domain: $\Omega=V$
- feature(representations)
  - scalar feature, whole graph
  - vector feature, per node
  - tensor feature, link or node pair

### a graph is an abstract object

how we describe a graph in computer

### manifolds



## invariant representations

- to recognize whole objects, we need to first recognize parts
  - This is why neural networks should be deep
- if we make the intermediate representations invariant, we lose critical information
- The relative pose of object parts contains critical information

## Eqivariant networks

- Feature vector space $\mathcal{X}_i$
- Maps $f_i$ between them
- A symmetry group $\mathscr{G}$
- Group representations $\rho_i$ of $\mathscr{G}$ for each $\mathcal{X}_i$

**Equivariance**
$$
f_i\circ \rho_{i-1}(g)=\rho_i(g)\circ f_i
$$
