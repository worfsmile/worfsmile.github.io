# GEOMETRIC PRIORSⅡ

## how to apply geometric structure to problem

ML goal: Learn an unknown function $f^*:\mathcal{X}\rarr\R$

transformation in domain can lifted to linear transformations in domain

## Invariant Function Classes

Recall our learning setup

$f^*:\mathcal{X}(\Omega)\rarr\R$

$\mathcal{F}:$hypothesis class

Promise: $f^*$ is $\mathscr{G}$-invariant

for all $x\in \mathcal{X}(\Omega), g\in\mathscr{G},f^*(g.x)=f^*(x)$

### smoothing operator

$$
S_{\mathscr{G}}f\overset{def}{=}\frac{1}{|\mathscr{G}|}\sum_{g\in\mathscr{G}}f\circ g
$$

$$
S_{\mathscr{G}}\mathcal{F}\overset{def}{=}\{S_{\mathscr{G}}f;f\in\mathcal{F}\}
$$



- Approximation error not affected

$$
inf_{f\in\mathcal{F}}||f-f^*||^2=inf_{f\in S_{\mathscr{G}}\mathcal{F}}||f-f^*||^2
$$

note that $S_{\mathscr{G}}\mathcal{F}$ is an orthogonal projection operator in $L^2(\mathcal{X})$

then $f$ can be divide into $S_{\mathscr{G}}f$ and $(I-S_{\mathscr{G}})f$

proof of GPT[appendix](some_appendix.md) 

- Statistical error reduced

Lipschitz class $\mathcal{F}$. $\mathscr{G}$-smoothed
$$
\mathbb{E}\mathcal{R}(\widetilde{f})\underset{\sim}{<}(|\mathscr{G}|n)^{-\frac{1}{d}}
$$

## scale separation

basics of multiresolution analysis

Multiresolution Analysis(多分辨率分析)

filters are loclized in space

- scale separation prior

$$
f^*\approx\sum_ug(x_u)
$$

- Local Markov Random Fields

global information and local information

function and hierarchical

> 学习图像，图片的数量需要指数级别增长随着像素点的增加



element-wise non-linear function
$$
\rho:\mathcal{X}\rarr\mathcal{X},with\rho x(u)=\rho(x(u))
$$

### The GDL Blueprint

equivariant layer

local pooling layer

equivatiant layer

invariant global pooling

### Keys

assumption and architecture

from local to global

avoiding to process all information one by one, instead, we want to process related information(bias) together

