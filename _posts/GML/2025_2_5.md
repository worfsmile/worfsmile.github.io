# Geometric Deep Learning Introduction

EUCLID=GEOMETRY

other geometry

geometry implies invariance and symmetries. The properties that remain unchanged under some class of transformations. 

physics=symmetry

we study geometric deep learning to provide a common mathematical framework to derive the most successful neural network architectures and second to give a constructive procedure to build future architectures in a principled way.

Let’s discuss neural networks briefly

1. universal approximation
2. high dimensionality

we can consider the arithmetic structure of neural network

from input image to input vector then put it into neural network 

we can think about the figure number three and put it in different place in the picture or rotate it in different angle. Then we need many picture to make a neural network fitting this num.

From the above discussion, it makes sense to think of bias in terms of variance and invariance.

scale separation and propagate them

equivariant and local pooling

5G of geometry machine learning

## graph

social network

permutation

permutation invariance

permutation equivariant

loacal neighbourhood

local function

### WL test

When checking for graph isomorphism, we can determine it by using the equation:

$PAP^* = A'$, 

A well-known method for this is the WL test. In the 1-hop WL test, we aggregate the neighbors of each node and use an identity function to map the node to a new state. After the process converges, we compare the frequency of node states between the two graphs. If the frequencies match, we can conclude that the graphs are isomorphic.

note that it is a necessary but insufficient condition

### some function

here we can see some function about graph neural network

there are mainly three architectures

the first flavor is convolutional
$$
f(X_i)=\phi(X_i,\underset{j\in \mathcal{N}_i}{\square}c_{ij}\phi(X_j))
$$
where $c_{ij}$ can be interpreted as the importance of node j to node i

the second flavor is  based on a tension when the aggregation coefficients now depend on the features themselves and known as attention.
$$
f(X_i)=\phi(X_i,\underset{j\in \mathcal{N}_i}{\square}attention_{ij}\phi(X_j))
$$
And in most general flavor we have a nonlinear function dependent on both feature vectors of node i and j whose output can be regarded as message. And it is known as message passing.
$$
f(X_i)=\phi(X_i,\underset{j\in \mathcal{N}_i}{\square}\phi(X_i,X_j))
$$
in typical GNN

we have several permutation-equivariant layer and some local and global pooling layer for a graph to derive features

### some instuition for graph

firstly, node points are a simple set

and a graph is set+edge

when a graph is complete, aggregation makes no sense

and it may make sense that embedding node positions for GNN Graph Substructure Network

and here is a useful method to decouple computation graph from the input graph

dynamic GNN

### manifold learning

1. represent data structure as a graph
2. graph operations in algebra

### convolution

convolution means translation symmetry

DFT

it is meaningful to understand that fourier transform comes from the fundamental principle of symmetry. 

either multiplying by a circuit matrix risk corresponds to sliding a filter along our signal or in the fourier domain as element wise product of the Fourier transforms of the signal and the filter

## group

firstly let's take a look at this formula:
$$
(x*\phi)(u)=<x,T_u\phi>=\int^{+\infty}_{-\infty}x(v)\phi(u-v)dv
$$

## symmetry and geometric

### history

- Euclid's Monopoly

- Riemann pointed out that

> Manifolds in which, as in the plane and in space, the line-element may be reduced to the form as L2 norm, are therefore only a particular case of the manifolds to be here investigated; they require a special name, and therefore these manifolds in which the square of the lin-element may be expressed as the sum of the squares of complete differentials I will call flat.

From this, we arrive at various geometries, including Euclidean, hyperbolic, affine, elliptic, and projective geometries.

- group theory

Noether Theorem

> every (differentiable) symmetry of the action of a physical system (with conservative forces) has a corresponding conservation law

- gauge invariance

- unification of forces

physics and deep learning

let's take a look at the zoo of neural network architectures. there are famous architectures including GNN, CNN, DeepSets, RNN and transformer

- The Erlangen Programme of ML Geometric Deep Learning

> We can obtain helpful insights from this [website](https://geometricdeeplearning.com/)

## more for machine learning

Here, we focus on deeper intuitions for machine learning, which offer great insights from industry experts.

1. supervised ML = Function Approximation
2. universal approximation
3. The curse of dimensionality, when viewing the learning process as a transition from prior to posterior, becomes a challenge due to the increasing number of dimensions.

4. image and shift invariance from data

   - When we transform an image from its pixel-based representation to a d-dimensional vector, we destroy its local spatial connectivity, making it more susceptible to simple image operations such as translation, rotation, and scaling.

     > data argument

   - local share way

5. grids and graphs

   - social networks
   - interaction networks
   - meshes
   - functional networks

6. geometric priors and symmetry prior

   - signals $\mathcal{X}(\Omega)$
   - symmetric group $\mathcal{G}$

7. Invariant functions in Image Classification$f(\rho(\mathcal{g})x) = f(x)$ where $\rho$ can apply transformations like color, position, rotation, etc., enabling the classifier to focus on the shape. And we can apply scale separation as a prior to eliminate the effect of position.

Then, here’s the blueprint for geometric deep learning: several equivariant layers followed by local pooling layers, then more equivariant layers, and ending with a global invariant layer.