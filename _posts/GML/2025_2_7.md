# High Dimensional Learning

## statistical learning

here are some high level instuition of statistical learning

and **in general statistical learning is plagued with exponential complexity (we need to make assumptions in order to learn)**

### data distribution

1. $\{(x_i,y_i)\}$ and let $x\sim v$
2. $y_i=f(x_i)$
3. $P(y=1|x)$
4. assumptions on both $v$ and $f$

### approximation model

1. hypothesis class

   - polynomials of degree up to k
   - neural networks of given architecture

   the hypothesis class is organized in terms of a (non-negative) complexity measure (measure the hypothesis space)

2. Model complexity refers to the capacity of the hypothesis, which can be intuitively expressed as a small region within the hypothesis function space.

3. interpolation form: $argmin_{f} \tau(f)\ s.t.\hat{\mathcal{R}}(f)=0$

### error metric

1. loss function
2. population loss and empirical loss
3. unbiased estimator
4. Rademacher complexities, Rademacher complexity measures how well a hypothesis class can "shatter" or "fit" random noise which used as $\sigma^2$



Here we have error metrics, including population loss, which is defined by the expectation of loss with respect to the underlying distribution of the data, and empirical loss, which is defined as a statistic of the samples — specifically, the mean loss over the samples. If the samples are drawn independently and identically distributed (i.i.d.) from the underlying population, the empirical loss is an unbiased estimator of the population loss. That is, the expected value of the empirical loss equals the population loss with $\sigma^2$

empirical risk minimization(ERM), constrained form:

$\hat{f}_{\delta}=arg \underset{f\in\mathcal{F}_{\delta}}{min}\hat{R}(f)$

asssume the hypothesis space is convex and constrant

We can only consider the functions that can be expressed and achieved through optimization. Therefore, we use $\mathcal{F}_\delta$ to uniformly control fluctuations.

first let's clarify some signal.

$R(f)=\mathbb{E}[\mathcal{l}(f(x), f^*(x))]$ where $f^*(x)$ generate the true label

$\hat{R}(f)=\frac{1}{n}\sum_i\mathcal{l}(f(x_i),f^*(x_i))$

$\hat{f}$: arbitrary $\hat{f}\in \mathcal{F}_{\delta}$
$$
\begin{align}
&\mathcal{R}(\hat{f})-inf_{f\in\mathcal{F}}R(f)\\
&=(R(\hat{f})-inf_{f\in\mathcal{F}_\delta(f)}R(f))+(inf_{f\in\mathcal{F}_\delta(f)}R(f)-inf_{f\in\mathcal{F}}R(f))\\
&=(\hat{R}(\hat{f})-inf_{f\in\mathcal{F}_\delta(f)}\hat{R}(f))+(R({\hat{f}})-\hat{R}(\hat{f}))+(inf_{f\in\mathcal{F}_\delta(f)}\hat{R}(f)-inf_{f\in\mathcal{F}_\delta(f)}R(f))+\epsilon_{appr}\\
&\leq \epsilon_{opt}+2sup_{f\in\mathcal{F}_\delta(f)}|R(f)-\hat{R}(f)|-+\epsilon_{appr}\\
&=\epsilon_{opt}+\epsilon_{stat}+\epsilon_{appr}\\
&\mathcal{R}(\hat{f})\leq inf_{f\in\mathcal{F}}R(f)+\epsilon_{opt}+\epsilon_{stat}+\epsilon_{appr}\tag{1}
\end{align}
$$

### estimation function

>It is included somwhere above.



## to high dimension

Now, let's discuss based on equation (1). We can see that the risk can be bounded by four terms, and we will analyze them one by one.

- $inf_{f\in\mathcal{F}}R(f)$ will be small if the hypothesis space is very large， allowing us to achieve the best solution.
- $\epsilon_{opt}$ will be small if hypothesis space is simple, meaning we can achieve the best solution in terms of empirical risk.
- $\epsilon_{stat}$ will be small if the $f\in\mathcal{F}_\delta(f)$ is small
- $\epsilon_{appr}=inf_{f\in\mathcal{F}_{\delta}}R(f)$ will be small if $f^*$ has small complexity, meaning that we can achieve $f^*$

Now, let's consider how to incorporate all these factors, and we can derive such a requirement:

We aim to construct a hypothesis that is as simple and well-regularized as possible, with the goal of making the problem intuitive and ensuring the optimization process is efficient and smooth.

### the curse of dimensionality

something about dimension above is $\delta$

- thinking the problem in interpolation

something about Lipschitz Functions

- it shows that function has local proprety

samples and function dimension

- gradient-descent can efficiently find local minima in high-dimensions
- provided the ERM has 'no bad' local minima, no dimension dependence (up to log factors) in iteration complexity.



Curse in Approximation
$$
\mathcal{F}=\{f(x=\sum_{j\leq m}a_j\rho(w_j^Tx+b_j)\}\\
\gamma(f)=m(number\ of\ neurons),\gamma(f)=\sum|a_j|(||w_j||+|b_j|)
$$

- Universal Approximation Theorem and Quantitative Rates?
  - rate are cursed in the Sobolev Class: if $f\in \mathcal{H}^s$, then $inf_{g\in\mathcal{F}}sup_{x\in K}|f(x)-g(x)|=\Theta(m^{-\frac{s}{d}})$
  - Barron Class: if $f\in\mathcal{H}^1$ is such that $\int|\hat{f}(w)|·||w||^2dw\leq\infty$, curse is avoided: $inf_{g\in\mathcal{F}sup_{x\in K}}|f(x)-g(x)|=O(m^{-1})$

## summary

- Lipschitz class is too large: statistical error cursed by dimension

- Smooth Sobolev/Barron classes are too small: approximation error cursed by dimension

  >**不属于Sobolev类的函数**通常具有以下特征：
  >
  >- 不连续性（特别是强跳跃或无限阶不连续性）。
  >- 导数不定义或不满足可积性条件。
  >- 无界性或者在某些区域内无法积分。
  >- 极端不光滑或者不符合弱导数的要求。

  > **Barron Classes** 是指具有特定结构和光滑性的函数空间，主要用于表示具有良好近似性质的函数。这些类被广泛用于神经网络的理论研究，特别是在泛化能力和优化方面。
  >
  > 不属于Barron Classes的函数通常
  >
  > 具有不连续性或跳跃。
  >
  > 具有过高的复杂度或不可分解性。
  >
  > 极端不规则或稀疏的函数表示。
  >
  > 使用不连续的激活函数。
  >
  > 不能通过有限的神经网络或模型进行有效表示或逼近。

So we need more adapted function spaces.

### Towards Geometric Function Space

signals and function
