---
title: 'trellis3d-code train1'
date: 2025-10-08
permalink: /posts/2025/10/blog-post-8/
tags:
series: TrellisCode
---

# Train `slat_vae_enc_dec_gs_swin8_B_64l8_fp16`
======

- dataset input
- model architecture and data transform
- multi-gpu training

## dataset deal

```python
class SparseFeat2Render(StandardDatasetBase):
    """
    SparseFeat2Render dataset.
    
    Args:
        roots (str): paths to the dataset
        image_size (int): size of the image
        model (str): model name
        resolution (int): resolution of the data
        min_aesthetic_score (float): minimum aesthetic score
        max_num_voxels (int): maximum number of voxels
    """
    def __init__(
        self,
        roots: str,
        image_size: int,
        model: str = 'dinov2_vitl14_reg',
        resolution: int = 64,
        min_aesthetic_score: float = 5.0,
        max_num_voxels: int = 32768,
    ):
        self.image_size = image_size    # 512
        self.model = model  # 'dinov2_vitl14_reg'
        self.resolution = resolution  # 64
        self.min_aesthetic_score = min_aesthetic_score  # 5.0
        self.max_num_voxels = max_num_voxels  # 32768
        self.value_range = (0, 1)
        
        super().__init__(roots)

class StandardDatasetBase(Dataset):
    """
    Base class for standard datasets.

    Args:
        roots (str): paths to the dataset
    """

    def __init__(self,
        roots: str,
    ):
        super().__init__()
        self.roots = roots.split(',')   # dataset roots
        self.instances = []
        self.metadata = pd.DataFrame()
        
        self._stats = {}
        for root in self.roots:
            key = os.path.basename(root)
            self._stats[key] = {}
            metadata = pd.read_csv(os.path.join(root, 'metadata.csv'))
            self._stats[key]['Total'] = len(metadata)
            metadata, stats = self.filter_metadata(metadata)
            self._stats[key].update(stats)
            self.instances.extend([(root, sha256) for sha256 in metadata['sha256'].values])
            metadata.set_index('sha256', inplace=True)
            self.metadata = pd.concat([self.metadata, metadata])

# {"feats": [B, [SparseTensor[N, 1024]]], features 获得, coords 化为了整数
#"image": [B, 3, H, W], "alpha": [B, H, W], "extrinsics": [B, 4, 4] "intrinsics": [B, 3, 3]} render获得, 其中image是n个view中随机采样一张

# dataset: 保存数据的路径
# __getitem__: 从文件中读取数据
```

## 核心数据结构

```python
class SparseTensor:
    """
    Sparse tensor with support for both torchsparse and spconv backends.
    
    Parameters:
    - feats (torch.Tensor): Features of the sparse tensor.
    - coords (torch.Tensor): Coordinates of the sparse tensor.
    - shape (torch.Size): Shape of the sparse tensor.
    - layout (List[slice]): Layout of the sparse tensor for each batch
    - data (SparseTensorData): Sparse tensor data used for convolusion

    NOTE:
    - Data corresponding to a same batch should be contiguous.
    - Coords should be in [0, 1023]
    """

    def replace(self, feats: torch.Tensor, coords: Optional[torch.Tensor] = None) -> 'SparseTensor':
        new_shape = [self.shape[0]]
        new_shape.extend(feats.shape[1:])
        if BACKEND == 'torchsparse':
            new_data = SparseTensorData(
                feats=feats,
                coords=self.data.coords if coords is None else coords,
                stride=self.data.stride,
                spatial_range=self.data.spatial_range,
            )
            new_data._caches = self.data._caches
        elif BACKEND == 'spconv':
            new_data = SparseTensorData(
                self.data.features.reshape(self.data.features.shape[0], -1),
                self.data.indices,
                self.data.spatial_shape,
                self.data.batch_size,
                self.data.grid,
                self.data.voxel_num,
                self.data.indice_dict
            )
            new_data._features = feats
            new_data.benchmark = self.data.benchmark
            new_data.benchmark_record = self.data.benchmark_record
            new_data.thrust_allocator = self.data.thrust_allocator
            new_data._timer = self.data._timer
            new_data.force_algo = self.data.force_algo
            new_data.int8_scale = self.data.int8_scale
            if coords is not None:
                new_data.indices = coords
        new_tensor = SparseTensor(new_data, shape=torch.Size(new_shape), layout=self.layout, scale=self._scale, spatial_cache=self._spatial_cache)
        return new_tensor
```

## Train

初始化

```python
# trainer/vae/structured_latent_vae_gaussian.py/SLatVaeGaussianTrainer
@torch.no_grad()
def run_snapshot(
    self,
    num_samples: int,   # 64 一次run_snapshot对比多少个体素
    batch_size: int,
    verbose: bool = False,
) -> Dict:
    dataloader = DataLoader(
        copy.deepcopy(self.dataset),
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,
        collate_fn=self.dataset.collate_fn if hasattr(self.dataset, 'collate_fn') else None,
    )

    # inference
    ret_dict = {}
    gt_images = []
    exts = []
    ints = []
    reps = []
    for i in range(0, num_samples, batch_size): # 这个i貌似是体素
        batch = min(batch_size, num_samples - i)
        data = next(iter(dataloader))
        args = {k: v[:batch].cuda() for k, v in data.items()}
        gt_images.append(args['image'] * args['alpha'][:, None])
        exts.append(args['extrinsics'])
        ints.append(args['intrinsics'])
        z = self.models['encoder'](args['feats'], sample_posterior=True, return_raw=False)
        reps.extend(self.models['decoder'](z))
    gt_images = torch.cat(gt_images, dim=0) # [64, 3, 512, 512]
    ret_dict.update({f'gt_image': {'value': gt_images, 'type': 'image'}})

    # render single view
    exts = torch.cat(exts, dim=0)
    ints = torch.cat(ints, dim=0)
    self.renderer.rendering_options.bg_color = (0, 0, 0)
    self.renderer.rendering_options.resolution = gt_images.shape[-1]
    render_results = self._render_batch(reps, exts, ints)
    ret_dict.update({f'rec_image': {'value': render_results['color'], 'type': 'image'}})

    # render multiview
    self.renderer.rendering_options.resolution = 512
    ## Build camera
    yaws = [0, np.pi / 2, np.pi, 3 * np.pi / 2]
    yaws_offset = np.random.uniform(-np.pi / 4, np.pi / 4)
    yaws = [y + yaws_offset for y in yaws]
    pitch = [np.random.uniform(-np.pi / 4, np.pi / 4) for _ in range(4)]

    ## render each view
    miltiview_images = []
    for yaw, pitch in zip(yaws, pitch):
        orig = torch.tensor([
            np.sin(yaw) * np.cos(pitch),
            np.cos(yaw) * np.cos(pitch),
            np.sin(pitch),
        ]).float().cuda() * 2
        fov = torch.deg2rad(torch.tensor(30)).cuda()
        extrinsics = utils3d.torch.extrinsics_look_at(orig, torch.tensor([0, 0, 0]).float().cuda(), torch.tensor([0, 0, 1]).float().cuda())
        intrinsics = utils3d.torch.intrinsics_from_fov_xy(fov, fov)
        extrinsics = extrinsics.unsqueeze(0).expand(num_samples, -1, -1)
        intrinsics = intrinsics.unsqueeze(0).expand(num_samples, -1, -1)
        render_results = self._render_batch(reps, extrinsics, intrinsics)
        miltiview_images.append(render_results['color'])

    ## Concatenate views
    miltiview_images = torch.cat([
        torch.cat(miltiview_images[:2], dim=-2),
        torch.cat(miltiview_images[2:], dim=-2),
    ], dim=-1)
    ret_dict.update({f'miltiview_image': {'value': miltiview_images, 'type': 'image'}})

    self.renderer.rendering_options.bg_color = 'random'
                                
    return ret_dict
```

训练

```python
# trainer/vae/structured_latent_vae_gaussian.py/SLatVaeGaussianTrainer
def training_losses(
    self,
    feats: SparseTensor,
    image: torch.Tensor,
    alpha: torch.Tensor,
    extrinsics: torch.Tensor,
    intrinsics: torch.Tensor,
    return_aux: bool = False,
    **kwargs
) -> Tuple[Dict, Dict]:
    """
    Compute training losses.

    Args:
        feats: The [N x * x C] sparse tensor of features.
        image: The [N x 3 x H x W] tensor of images.
        alpha: The [N x H x W] tensor of alpha channels.
        extrinsics: The [N x 4 x 4] tensor of extrinsics.
        intrinsics: The [N x 3 x 3] tensor of intrinsics.
        return_aux: Whether to return auxiliary information.

    Returns:
        a dict with the key "loss" containing a scalar tensor.
        may also contain other keys for different terms.
    """
    z, mean, logvar = self.training_models['encoder'](feats, sample_posterior=True, return_raw=True)
    reps = self.training_models['decoder'](z)
    self.renderer.rendering_options.resolution = image.shape[-1]
    render_results = self._render_batch(reps, extrinsics, intrinsics)     
    
    terms = edict(loss = 0.0, rec = 0.0)
    
    rec_image = render_results['color']
    gt_image = image * alpha[:, None] + (1 - alpha[:, None]) * render_results['bg_color'][..., None, None]
            
    if self.loss_type == 'l1':
        terms["l1"] = l1_loss(rec_image, gt_image)
        terms["rec"] = terms["rec"] + terms["l1"]
    elif self.loss_type == 'l2':
        terms["l2"] = l2_loss(rec_image, gt_image)
        terms["rec"] = terms["rec"] + terms["l2"]
    else:
        raise ValueError(f"Invalid loss type: {self.loss_type}")
    if self.lambda_ssim > 0:
        terms["ssim"] = 1 - ssim(rec_image, gt_image)
        terms["rec"] = terms["rec"] + self.lambda_ssim * terms["ssim"]
    if self.lambda_lpips > 0:
        terms["lpips"] = lpips(rec_image, gt_image)
        terms["rec"] = terms["rec"] + self.lambda_lpips * terms["lpips"]
    terms["loss"] = terms["loss"] + terms["rec"]

    terms["kl"] = 0.5 * torch.mean(mean.pow(2) + logvar.exp() - logvar - 1)
    terms["loss"] = terms["loss"] + self.lambda_kl * terms["kl"]
    
    reg_loss, reg_terms = self._get_regularization_loss(reps)
    terms.update(reg_terms)
    terms["loss"] = terms["loss"] + reg_loss
    
    status = self._get_status(z, reps)
    
    if return_aux:
        return terms, status, {'rec_image': rec_image, 'gt_image': gt_image}       
    return terms, status
```

## Model

window attention

```python
def calc_window_partition(
    tensor: SparseTensor,
    window_size: Union[int, Tuple[int, ...]],
    shift_window: Union[int, Tuple[int, ...]] = 0
) -> Tuple[torch.Tensor, torch.Tensor, List[int], List[int]]:
    """
    Calculate serialization and partitioning for a set of coordinates.

    Args:
        tensor (SparseTensor): The input tensor.
        window_size (int): The window size to use.
        shift_window (Tuple[int, ...]): The shift of serialized coordinates.

    Returns:
        (torch.Tensor): Forwards indices.
        (torch.Tensor): Backwards indices.
        (List[int]): Sequence lengths.
        (List[int]): Sequence batch indices.
    """
    DIM = tensor.coords.shape[1] - 1
    shift_window = (shift_window,) * DIM if isinstance(shift_window, int) else shift_window
    window_size = (window_size,) * DIM if isinstance(window_size, int) else window_size
    shifted_coords = tensor.coords.clone().detach()
    shifted_coords[:, 1:] += torch.tensor(shift_window, device=tensor.device, dtype=torch.int32).unsqueeze(0)

    MAX_COORDS = shifted_coords[:, 1:].max(dim=0).values.tolist()
    NUM_WINDOWS = [math.ceil((mc + 1) / ws) for mc, ws in zip(MAX_COORDS, window_size)]
    OFFSET = torch.cumprod(torch.tensor([1] + NUM_WINDOWS[::-1]), dim=0).tolist()[::-1]

    shifted_coords[:, 1:] //= torch.tensor(window_size, device=tensor.device, dtype=torch.int32).unsqueeze(0)
    shifted_indices = (shifted_coords * torch.tensor(OFFSET, device=tensor.device, dtype=torch.int32).unsqueeze(0)).sum(dim=1)
    fwd_indices = torch.argsort(shifted_indices)
    bwd_indices = torch.empty_like(fwd_indices)
    bwd_indices[fwd_indices] = torch.arange(fwd_indices.shape[0], device=tensor.device)
    seq_lens = torch.bincount(shifted_indices)
    seq_batch_indices = torch.arange(seq_lens.shape[0], device=tensor.device, dtype=torch.int32) // OFFSET[0]
    mask = seq_lens != 0
    seq_lens = seq_lens[mask].tolist()
    seq_batch_indices = seq_batch_indices[mask].tolist()

    return fwd_indices, bwd_indices, seq_lens, seq_batch_indices

def sparse_windowed_scaled_dot_product_self_attention(
    qkv: SparseTensor,
    window_size: int,
    shift_window: Tuple[int, int, int] = (0, 0, 0)
) -> SparseTensor:
    """
    Apply windowed scaled dot product self attention to a sparse tensor.

    Args:
        qkv (SparseTensor): [N, *, 3, H, C] sparse tensor containing Qs, Ks, and Vs.
        window_size (int): The window size to use.
        shift_window (Tuple[int, int, int]): The shift of serialized coordinates.
        shift (int): The shift to use.
    """
    assert len(qkv.shape) == 4 and qkv.shape[1] == 3, f"Invalid shape for qkv, got {qkv.shape}, expected [N, *, 3, H, C]"

    serialization_spatial_cache_name = f'window_partition_{window_size}_{shift_window}'
    serialization_spatial_cache = qkv.get_spatial_cache(serialization_spatial_cache_name)
    if serialization_spatial_cache is None:
        fwd_indices, bwd_indices, seq_lens, seq_batch_indices = calc_window_partition(qkv, window_size, shift_window)
        qkv.register_spatial_cache(serialization_spatial_cache_name, (fwd_indices, bwd_indices, seq_lens, seq_batch_indices))
    else:
        fwd_indices, bwd_indices, seq_lens, seq_batch_indices = serialization_spatial_cache

    M = fwd_indices.shape[0]
    T = qkv.feats.shape[0]
    H = qkv.feats.shape[2]
    C = qkv.feats.shape[3]
    
    qkv_feats = qkv.feats[fwd_indices]      # [M, 3, H, C]

    if DEBUG:
        start = 0
        qkv_coords = qkv.coords[fwd_indices]
        for i in range(len(seq_lens)):
            seq_coords = qkv_coords[start:start+seq_lens[i]]
            assert (seq_coords[:, 0] == seq_batch_indices[i]).all(), f"SparseWindowedScaledDotProductSelfAttention: batch index mismatch"
            assert (seq_coords[:, 1:].max(dim=0).values - seq_coords[:, 1:].min(dim=0).values < window_size).all(), \
                    f"SparseWindowedScaledDotProductSelfAttention: window size exceeded"
            start += seq_lens[i]

    if all([seq_len == window_size for seq_len in seq_lens]):
        B = len(seq_lens)
        N = window_size
        qkv_feats = qkv_feats.reshape(B, N, 3, H, C)
        if ATTN == 'xformers':
            q, k, v = qkv_feats.unbind(dim=2)                       # [B, N, H, C]
            out = xops.memory_efficient_attention(q, k, v)          # [B, N, H, C]
        elif ATTN == 'flash_attn':
            out = flash_attn.flash_attn_qkvpacked_func(qkv_feats)   # [B, N, H, C]
        else:
            raise ValueError(f"Unknown attention module: {ATTN}")
        out = out.reshape(B * N, H, C)                              # [M, H, C]
    else:
        if ATTN == 'xformers':
            q, k, v = qkv_feats.unbind(dim=1)                       # [M, H, C]
            q = q.unsqueeze(0)                                      # [1, M, H, C]
            k = k.unsqueeze(0)                                      # [1, M, H, C]
            v = v.unsqueeze(0)                                      # [1, M, H, C]
            mask = xops.fmha.BlockDiagonalMask.from_seqlens(seq_lens)
            out = xops.memory_efficient_attention(q, k, v, mask)[0] # [M, H, C]
        elif ATTN == 'flash_attn':
            cu_seqlens = torch.cat([torch.tensor([0]), torch.cumsum(torch.tensor(seq_lens), dim=0)], dim=0) \
                        .to(qkv.device).int()
            out = flash_attn.flash_attn_varlen_qkvpacked_func(qkv_feats, cu_seqlens, max(seq_lens)) # [M, H, C]

    out = out[bwd_indices]      # [T, H, C]

    if DEBUG:
        qkv_coords = qkv_coords[bwd_indices]
        assert torch.equal(qkv_coords, qkv.coords), "SparseWindowedScaledDotProductSelfAttention: coordinate mismatch"

    return qkv.replace(out)
```

```text
"encoder": {
    "name": "ElasticSLatEncoder",
    "args": {
        "resolution": 64,
        "in_channels": 1024,
        "model_channels": 768,
        "latent_channels": 8,
        "num_blocks": 12,
        "num_heads": 12,
        "mlp_ratio": 4,
        "attn_mode": "swin",
        "window_size": 8,
        "use_fp16": true
    }
},
"decoder": {
    "name": "ElasticSLatGaussianDecoder",
    "args": {
        "resolution": 64,
        "model_channels": 768,
        "latent_channels": 8,
        "num_blocks": 12,
        "num_heads": 12,
        "mlp_ratio": 4,
        "attn_mode": "swin",
        "window_size": 8,
        "use_fp16": true,
        "representation_config": {
            "lr": {
                "_xyz": 1.0,
                "_features_dc": 1.0,
                "_opacity": 1.0,
                "_scaling": 1.0,
                "_rotation": 0.1
            },
            "perturb_offset": true,
            "voxel_size": 1.5,
            "num_gaussians": 32,
            "2d_filter_kernel_size": 0.1,
            "3d_filter_kernel_size": 0.0009,
            "scaling_bias": 0.004,
            "opacity_bias": 0.1,
            "scaling_activation": "softplus"
        }
    }
}


Backbone: encoder
Parameters:

Name                                                                    Shape                           Type            Grad
input_layer.weight                                                      torch.Size([768, 1024])         torch.float32   True
input_layer.bias                                                        torch.Size([768])               torch.float32   True
blocks.0.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.0.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.0.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.0.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.0.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.0.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.0.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.0.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.1.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.1.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.1.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.1.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.1.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.1.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.1.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.1.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.2.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.2.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.2.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.2.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.2.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.2.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.2.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.2.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.3.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.3.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.3.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.3.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.3.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.3.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.3.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.3.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.4.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.4.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.4.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.4.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.4.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.4.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.4.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.4.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.5.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.5.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.5.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.5.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.5.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.5.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.5.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.5.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.6.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.6.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.6.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.6.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.6.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.6.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.6.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.6.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.7.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.7.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.7.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.7.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.7.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.7.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.7.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.7.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.8.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.8.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.8.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.8.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.8.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.8.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.8.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.8.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.9.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.9.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.9.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.9.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.9.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.9.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.9.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.9.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.10.attn.to_qkv.weight                                            torch.Size([2304, 768])         torch.float16   True
blocks.10.attn.to_qkv.bias                                              torch.Size([2304])              torch.float16   True
blocks.10.attn.to_out.weight                                            torch.Size([768, 768])          torch.float16   True
blocks.10.attn.to_out.bias                                              torch.Size([768])               torch.float16   True
blocks.10.mlp.mlp.0.weight                                              torch.Size([3072, 768])         torch.float16   True
blocks.10.mlp.mlp.0.bias                                                torch.Size([3072])              torch.float16   True
blocks.10.mlp.mlp.2.weight                                              torch.Size([768, 3072])         torch.float16   True
blocks.10.mlp.mlp.2.bias                                                torch.Size([768])               torch.float16   True
blocks.11.attn.to_qkv.weight                                            torch.Size([2304, 768])         torch.float16   True
blocks.11.attn.to_qkv.bias                                              torch.Size([2304])              torch.float16   True
blocks.11.attn.to_out.weight                                            torch.Size([768, 768])          torch.float16   True
blocks.11.attn.to_out.bias                                              torch.Size([768])               torch.float16   True
blocks.11.mlp.mlp.0.weight                                              torch.Size([3072, 768])         torch.float16   True
blocks.11.mlp.mlp.0.bias                                                torch.Size([3072])              torch.float16   True
blocks.11.mlp.mlp.2.weight                                              torch.Size([768, 3072])         torch.float16   True
blocks.11.mlp.mlp.2.bias                                                torch.Size([768])               torch.float16   True
out_layer.weight                                                        torch.Size([16, 768])           torch.float32   True
out_layer.bias                                                          torch.Size([16])                torch.float32   True

Number of parameters: 85817104
Number of trainable parameters: 85817104



Backbone: decoder
Parameters:

Name                                                                    Shape                           Type            Grad
input_layer.weight                                                      torch.Size([768, 8])            torch.float32   True
input_layer.bias                                                        torch.Size([768])               torch.float32   True
blocks.0.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.0.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.0.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.0.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.0.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.0.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.0.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.0.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.1.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.1.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.1.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.1.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.1.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.1.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.1.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.1.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.2.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.2.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.2.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.2.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.2.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.2.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.2.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.2.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.3.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.3.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.3.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.3.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.3.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.3.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.3.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.3.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.4.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.4.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.4.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.4.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.4.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.4.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.4.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.4.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.5.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.5.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.5.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.5.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.5.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.5.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.5.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.5.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.6.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.6.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.6.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.6.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.6.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.6.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.6.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.6.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.7.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.7.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.7.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.7.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.7.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.7.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.7.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.7.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.8.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.8.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.8.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.8.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.8.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.8.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.8.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.8.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.9.attn.to_qkv.weight                                             torch.Size([2304, 768])         torch.float16   True
blocks.9.attn.to_qkv.bias                                               torch.Size([2304])              torch.float16   True
blocks.9.attn.to_out.weight                                             torch.Size([768, 768])          torch.float16   True
blocks.9.attn.to_out.bias                                               torch.Size([768])               torch.float16   True
blocks.9.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.9.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.9.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.9.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.10.attn.to_qkv.weight                                            torch.Size([2304, 768])         torch.float16   True
blocks.10.attn.to_qkv.bias                                              torch.Size([2304])              torch.float16   True
blocks.10.attn.to_out.weight                                            torch.Size([768, 768])          torch.float16   True
blocks.10.attn.to_out.bias                                              torch.Size([768])               torch.float16   True
blocks.10.mlp.mlp.0.weight                                              torch.Size([3072, 768])         torch.float16   True
blocks.10.mlp.mlp.0.bias                                                torch.Size([3072])              torch.float16   True
blocks.10.mlp.mlp.2.weight                                              torch.Size([768, 3072])         torch.float16   True
blocks.10.mlp.mlp.2.bias                                                torch.Size([768])               torch.float16   True
blocks.11.attn.to_qkv.weight                                            torch.Size([2304, 768])         torch.float16   True
blocks.11.attn.to_qkv.bias                                              torch.Size([2304])              torch.float16   True
blocks.11.attn.to_out.weight                                            torch.Size([768, 768])          torch.float16   True
blocks.11.attn.to_out.bias                                              torch.Size([768])               torch.float16   True
blocks.11.mlp.mlp.0.weight                                              torch.Size([3072, 768])         torch.float16   True
blocks.11.mlp.mlp.0.bias                                                torch.Size([3072])              torch.float16   True
blocks.11.mlp.mlp.2.weight                                              torch.Size([768, 3072])         torch.float16   True
blocks.11.mlp.mlp.2.bias                                                torch.Size([768])               torch.float16   True
out_layer.weight                                                        torch.Size([448, 768])          torch.float32   True
out_layer.bias                                                          torch.Size([448])               torch.float32   True
```

## 代码结构

1. dataset
   1. 以文件名形式储存
   2. 在 `__getitem__` 读取并转化为数据
