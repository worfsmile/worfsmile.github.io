---
title: 'SparseTensor'
date: 2025-10-18
permalink: /posts/2025/10/blog-post-13/
tags:
series: TrellisCode
---

SparseTensor
======

## 稀疏张量

### 原因

假设样本形状为 $(N, C)$
其中 $N$ 不同, $C$ 为通道数相同

在做矩阵并行的广播计算时
一种方式是将样本进行padding到一样的形状

但是为了节省存储空间
往往不这样做

### 计算方式

通常将一个batch中的样本 $(N, C)$
进行标记为 $(i, C)$
其中 $i$ 为样本的索引
然后进行计算

### 卷积

卷积 = gather + matmul + scatter
1. 聚合邻居
2. 卷积核乘法, 和普通乘法一样
3. 累加到对应位置
通过 $i$ 索引可以进行相应计算

### 注意力

通过索引来标记做注意力的对象

### 补充

其实这种方式和图神经网络的计算方式一样
