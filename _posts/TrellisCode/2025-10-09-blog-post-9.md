---
title: 'trellis3d-code tricks'
date: 2025-10-09
permalink: /posts/2025/10/blog-post-9/
tags:
series: TrellisCode
---

一些用来记录数据, 多卡, 加速计算, 减少占据显存的方法

- SparseTensor
- window attention
- mp

```python
# manual_dist_demo.py
import os
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.optim as optim
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.multiprocessing as mp

def setup_dist(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group('nccl', rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def cleanup_dist():
    dist.destroy_process_group()

def demo(rank, world_size):
    setup_dist(rank, world_size)

    # 简单模型
    model = nn.Linear(10, 10).cuda(rank)
    ddp_model = DDP(model, device_ids=[rank])

    optimizer = optim.SGD(ddp_model.parameters(), lr=0.01)

    # 随机数据
    inputs = torch.randn(20, 10).cuda(rank)
    targets = torch.randn(20, 10).cuda(rank)

    # 训练循环
    for epoch in range(5):
        optimizer.zero_grad()
        outputs = ddp_model(inputs)
        loss = nn.MSELoss()(outputs, targets)
        loss.backward()
        optimizer.step()
        if rank == 0:
            print(f"Epoch {epoch} Loss: {loss.item()}")

    cleanup_dist()

if __name__ == "__main__":
    world_size = torch.cuda.device_count()
    mp.spawn(demo, args=(world_size,), nprocs=world_size, join=True)
```

```python
# accelerator_demo.py
import torch
import torch.nn as nn
import torch.optim as optim
from accelerate import Accelerator

# 初始化 Accelerator
accelerator = Accelerator(
    gradient_accumulation_steps=2,
    mixed_precision='fp16',  # 可选 'no' / 'bf16'
)

# 简单模型
model = nn.Linear(10, 10)
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 随机数据
inputs = torch.randn(20, 10)
targets = torch.randn(20, 10)

# 使用 Accelerator 封装模型、优化器和数据
model, optimizer, inputs, targets = accelerator.prepare(model, optimizer, inputs, targets)

# 训练循环
for epoch in range(5):
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = nn.MSELoss()(outputs, targets)
    accelerator.backward(loss)
    optimizer.step()
    if accelerator.is_main_process:
        print(f"Epoch {epoch} Loss: {loss.item()}")
```
